---
title: "Inspect Modelling Performance"
editor_options: 
  chunk_output_type: console
---

# Inspect Modelling Performance

```{r}
suppressPackageStartupMessages(library(tidyverse))
library(targets)
library(DT)
```

```{r, eval = TRUE}
knitr::opts_knit$set(root.dir = "../../")
```

## VIF

Maximum VIF for final model.

```{r}
df_cv_preds_and_coefs <- tar_read(df_cv_preds_and_coefs)

df_coefs <- df_cv_preds_and_coefs %>%
  select(year_start_act, category_id, pred) %>%
  mutate(coefs = map(pred, "coefs")) %>%
  select(- pred) %>%
  unnest(coefs) %>%
  unnest(coefs)

df_coefs %>%
  filter(year_start_act == 2019) %>%
  group_by(category_id) %>%
  summarise(max_vif = max(vif, na.rm = TRUE)) %>%
  knitr::kable()

```


## Perfermance Metrics

```{r, perf_metric}
df_perf <- tar_read(df_perf)
```

AUC and Brier were calculated for each test set and then the mean and standard error was calculated.

### AUC

```{r, auc}
df_perf %>%
  filter(.metric == "roc_auc") %>%
  knitr::kable(digits = 2)
```

### Brier

```{r brier}
df_perf %>%
  filter(.metric == "brier") %>%
  knitr::kable(digits = 2)
```


## Calibration

- pools all predictions for all test sets
- predicted test set probabilities have been split over 4 bins with a minimum of 100 observations per bin.
- actual observed frequencies were calculated for each bin and `prop.test()` was used for calculating confidence intervals.

### Linear Calibration

```{r}
tar_read(df_calib) %>%
  select(- plot_data) %>%
  mutate(delta = upper - lower) %>%
  select(category_id, lower, base_rate, upper, delta, intercept, slope) %>%
  knitr::kable(digits = 3)
```

### Plot

```{r fig.width=10, fig.height = 3}
tar_read(p_calib)
```

